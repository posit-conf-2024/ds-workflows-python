{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: today\n",
    "execute:\n",
    "    enabled: true\n",
    "format:\n",
    "    email:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this exercise we will perform feature engineering on our training set then train a model, deploy it and write a [model card](https://doi.org/10.1145/3287560.3287596) to provide reporting on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Setup\n",
    "\n",
    "These are just some preliminary steps for getting started with this section.\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Load any environment variables located in `.env`\n",
    "- Get the username for content deployed to Posit Connect\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import duckdb\n",
    "import hvplot.polars\n",
    "import pins\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import vetiver\n",
    "from dotenv import load_dotenv\n",
    "from posit.connect import Client\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from vetiver import VetiverModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(\".env\").exists():\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client() as client:\n",
    "    username = client.me.username\n",
    "\n",
    "print(f\"Connect username is: '{username}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Reading the data\n",
    "\n",
    "First we will read in the data prepared in [02-data-exploration-and-validation](../02-data-exploration-and-validation/notebook.ipynb) and take a quick look at it prepare for modeling.\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Read in and glimpse the vessel history data\n",
    "- Read in and glimpse the vessel verbose data\n",
    "- Read in and glimpse the weather data\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect('md:', read_only=True)\n",
    "database_name = \"washington_ferries\"\n",
    "con.execute(f\"USE {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history = con.sql(\"SELECT * FROM vessel_history\").pl()\n",
    "vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose = con.sql(\"SELECT * FROM vessel_verbose\").pl()\n",
    "vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = con.sql(\"SELECT * FROM terminal_weather\").pl()\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Feature Engineering\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Join the `vessel_history`, `vessel_verbose` and `weather` data into a form useful for modeling\n",
    "- Transform the columns in new ones we can use for modeling\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute a departure delay based on the difference between the actual and scheduled departure times and derive new features representing the day of the week and hour of departure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = vessel_history.with_columns(\n",
    "    (pl.col(\"ActualDepart\") - pl.col(\"ScheduledDepart\"))\n",
    "    .dt.total_seconds()\n",
    "    .alias(\"Delay\"),\n",
    "    pl.col(\"Date\").dt.weekday().alias(\"Weekday\"),\n",
    "    pl.col(\"Date\").dt.hour().alias(\"Hour\"),\n",
    ").drop(\"EstArrival\")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at `Delay` shows it is very skewed with a lot of values near zero and some even less than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined.hvplot.hist(\"Delay\", bin_range=(-1800, 7200), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not interested in negative values here, we'll clip the values below zero. We can then log them, turning into it a nicer, more \"normal\" distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.select(\n",
    "    pl.exclude(\"Delay\"),\n",
    "    pl.col(\"Delay\").map_elements(lambda x: max(x, 1), return_dtype=pl.Float64).log().alias(\"LogDelay\")\n",
    ")\n",
    "\n",
    "trips_combined.hvplot.hist(\"LogDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data describing the individual vessels by only keeping the columns we're interested in and extracting the year from `YearBuilt` and `YearRebuilt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_info = vessel_verbose.select(\n",
    "    pl.col(\"VesselName\"),\n",
    "    pl.col(\"ClassName\"),\n",
    "    # we can also select multiple columns in one `pl.col(...)`\n",
    "    pl.col(\n",
    "        \"SpeedInKnots\",\n",
    "        \"EngineCount\",\n",
    "        \"Horsepower\",\n",
    "        \"MaxPassengerCount\",\n",
    "        \"PassengerOnly\",\n",
    "        \"FastFerry\",\n",
    "        \"PropulsionInfo\",\n",
    "    ),\n",
    "    pl.col(\"YearBuilt\", \"YearRebuilt\").dt.year(),\n",
    ")\n",
    "\n",
    "vessel_info.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trips and vessel data are joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.join(\n",
    "    vessel_info, left_on=\"Vessel\", right_on=\"VesselName\", how=\"left\", coalesce=True\n",
    ")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining in the weather data is a little harder since the weather data has a temporal resolution of one-hour whereas the trips can leave at any arbitrary time, making a naive join on timestamps not work. Here, we round the the departure time of the trips to the nearest hour and then join on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.select(\n",
    "    pl.col(\n",
    "        \"time\",\n",
    "        \"weather_code\",\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"cloud_cover\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"wind_gusts_10m\",\n",
    "        \"terminal_name\",\n",
    "    )\n",
    ")\n",
    "\n",
    "trips_combined = (\n",
    "    trips_combined.with_columns(pl.col(\"Date\").dt.round(\"1h\").alias(\"time\"))\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"departing_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Departing\", \"time\"],\n",
    "        right_on=[\"departing_terminal_name\", \"departing_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"arriving_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Arriving\", \"time\"],\n",
    "        right_on=[\"arriving_terminal_name\", \"arriving_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .select(pl.exclude(\"time\"))\n",
    ")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how many `null`s there are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the drop the `null`s except for `YearRebuilt` since we expect to have weather data available. We can expect `YearRebuilt` in the case of vessels that have not yet been rebuilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.drop_nulls(subset=cs.exclude(\"YearRebuilt\"))\n",
    "\n",
    "trips_combined.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we enumerate all our numerical and categorical features. This will be needed later in setting up preprocessing for our model. For the categorical features we take a count of how often each category occurs to make sure our model isn't dependant on categories that are uncommon in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"SpeedInKnots\",\n",
    "    \"EngineCount\",\n",
    "    \"Horsepower\",\n",
    "    \"MaxPassengerCount\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRebuilt\",\n",
    "    \"departing_temperature_2m\",\n",
    "    \"departing_cloud_cover\",\n",
    "    \"departing_wind_speed_10m\",\n",
    "    \"departing_wind_direction_10m\",\n",
    "    \"departing_wind_gusts_10m\",\n",
    "    \"arriving_temperature_2m\",\n",
    "    \"arriving_cloud_cover\",\n",
    "    \"arriving_wind_speed_10m\",\n",
    "    \"arriving_wind_direction_10m\",\n",
    "    \"arriving_wind_gusts_10m\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Vessel\",\n",
    "    \"Weekday\",\n",
    "    \"Hour\",\n",
    "    \"Departing\",\n",
    "    \"Arriving\",\n",
    "    \"ClassName\",\n",
    "    \"PropulsionInfo\",\n",
    "    \"departing_weather_code\",\n",
    "    \"arriving_weather_code\",\n",
    "]\n",
    "\n",
    "for cf in categorical_features:\n",
    "    print(\n",
    "        f\"feature: '{cf}', count:\",\n",
    "        trips_combined.group_by(cf).agg(pl.len()).sort(\"len\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the weather codes don't occur as frequently as would be ideal and decide to keep note of which codes these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_count_weather_codes = set(\n",
    "    [\n",
    "        *trips_combined.group_by(\"departing_weather_code\")\n",
    "        .agg(pl.len())\n",
    "        .sort(\"len\")\n",
    "        .filter(pl.col(\"len\") < 300)[\"departing_weather_code\"]\n",
    "        .to_list(),\n",
    "        *trips_combined.group_by(\"arriving_weather_code\")\n",
    "        .agg(pl.len())\n",
    "        .sort(\"len\")\n",
    "        .filter(pl.col(\"len\") < 300)[\"arriving_weather_code\"]\n",
    "        .to_list(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "low_count_weather_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These less common weather codes are binned into their own category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_weather_codes(code):\n",
    "    return \"other\" if code in low_count_weather_codes else str(code)\n",
    "\n",
    "\n",
    "trips_combined = trips_combined.with_columns(\n",
    "    pl.col(\"departing_weather_code\").map_elements(\n",
    "        recode_weather_codes, return_dtype=pl.String\n",
    "    ),\n",
    "    pl.col(\"arriving_weather_code\").map_elements(\n",
    "        recode_weather_codes, return_dtype=pl.String\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some unnecessary columns are dropped and the `Date` column is turned into a proper date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.select(\n",
    "    cs.exclude(\"ScheduledDepart\", \"ActualDepart\")\n",
    ").with_columns(pl.col(\"Date\").dt.date())\n",
    "\n",
    "trips_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Model Training\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "Define a `scikit-learn` pipeline that\n",
    "\n",
    "- Transform the data for the model to ingest\n",
    "- Trains a gradient boosted machine model to predict the logged departure delay\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a column transformer is defined to preprocess our data. Numerical and categorical columns are treated differently with the former being passed through as-is and the latter being one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        # this just passes the variables through as-is\n",
    "        (\"numeric_features\", \"passthrough\", numeric_features),\n",
    "        # this one-hot encodes the variables\n",
    "        (\"categorical_features\", OneHotEncoder(), categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define our actual model, a [gradient boosted machine](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting) akin to [LightGBM](https://github.com/Microsoft/LightGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = HistGradientBoostingRegressor(verbose=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model requires non-sparse data, so a custom transformer is defined to take a sparse dataset and turn it into a dense one. See [toarray](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None, **params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transformers and model are combined into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        (\"column-transformer\", column_transformer),\n",
    "        (\"densify\", DenseTransformer()),\n",
    "        (\"regressor\", regressor),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set aside more recent for [model monitoring](../04-model-monitoring/monitoring_dashboard.qmd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = trips_combined.filter(\n",
    "    pl.col(\"Date\") < (datetime.date.today() - datetime.timedelta(weeks=4))\n",
    ")\n",
    "\n",
    "monitoring_data = trips_combined.filter(\n",
    "    pl.col(\"Date\") >= (datetime.date.today() - datetime.timedelta(weeks=4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect('md:', read_only=False)\n",
    "database_name = \"washington_ferries\"\n",
    "con.execute(f\"USE {database_name}\")\n",
    "\n",
    "table_name = \"monitoring_data\"\n",
    "con.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "con.sql(f'CREATE TABLE {table_name} as SELECT * FROM {table_name}') \n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the remaining data is split into a training set and testing set for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_test_data.drop(\"LogDelay\", \"Date\")\n",
    "y = train_test_data[\"LogDelay\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(f\"Number of rows of training data: {X_train.shape[0]}\")\n",
    "print(f\"Number of rows testing data:  {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect('md:', read_only=False)\n",
    "database_name = \"washington_ferries\"\n",
    "con.execute(f\"USE {database_name}\")\n",
    "\n",
    "table_name = \"x_test\"\n",
    "con.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "con.sql(f'CREATE TABLE {table_name} as SELECT * FROM X_test') \n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our model is trained. This also trains the encodings for our one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train.to_pandas(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Model Deployment\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Deploy the model using `vetiver` and `pins` onto Posit Connect\n",
    "- Deploy an API around the model onto Posit\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is wrapped up in a [`VetiverModel` object for serving it](https://rstudio.github.io/vetiver-python/stable/reference/VetiverModel.html#vetiver.VetiverModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VetiverModel(\n",
    "    model, model_name=f\"{username}/ferry_delay\", prototype_data=X.to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now deployed to Connect, making it available to use elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_board = pins.board_connect(allow_pickle_read=True)\n",
    "vetiver.vetiver_pin_write(model_board, model=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to more easily use our model, we need to define an API that loads and serves it. This API will also be deployed to Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"api\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# vetiver.write_app(model_board, f\"{username}/ferry_delay\", file=\"api/app.py\")\n",
    "shutil.copy2(\"requirements.txt\", \"api/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "api_guid = os.getenv(\"API_GUID\")\n",
    "if api_guid is None:\n",
    "    print(\"Creating a new deployment\")\n",
    "    !rsconnect deploy fastapi --entrypoint app:api --title \"{username}/ferry_delay_vetiver\" api/\n",
    "else:\n",
    "    print(f\"Updating existing deployment {api_guid}\")\n",
    "    !rsconnect deploy fastapi --app-id \"{api_guid}\" --entrypoint app:api --title \"{username}/ferry_delay_vetiver\" api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Model Card\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Use a model card to describe various metrics for how the model performs\n",
    "- Deploy the card to Connect\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetiver.templates.model_card()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
