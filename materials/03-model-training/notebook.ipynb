{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: today\n",
    "execute:\n",
    "    enabled: true\n",
    "format:\n",
    "    email:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this exercise we will train a model on the ferries and weather sets from earlier today. We'll be using a mix of `polars` and `scikit-learn` for some feature engineering and preprocessing of the data. The model will be deployed to and served from [Posit Connect](https://pub.ferryland.posit.team/) using [`pins`](https://rstudio.github.io/pins-python/) and [`vetiver`](https://rstudio.github.io/vetiver-python/stable/). For this section, we'll be using a [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#) model to predict the delay in ferry departures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First we'll load our environment variables from `.env` file and get our Connect username using the [Posit SDK for Python](https://github.com/posit-dev/posit-sdk-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env.\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if Path(\".env\").exists():\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Connect username.\n",
    "from posit.connect import Client\n",
    "\n",
    "connect_url = os.environ[\"CONNECT_SERVER\"]\n",
    "connect_api_key = os.environ[\"CONNECT_API_KEY\"]\n",
    "\n",
    "with Client(url=connect_url, api_key=connect_api_key) as client:\n",
    "    username = client.me.username\n",
    "\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Reading the data\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Read in and glimpse the vessel history data\n",
    "- Read in and glimpse the vessel verbose data\n",
    "- Read in and glimpse the weather data\n",
    "\n",
    "### üßë‚Äçüíª Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "db_uri = os.environ[\"DATABASE_URI_PYTHON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_history_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_history.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_verbose_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_verbose.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_weather_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Feature Engineering\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Join the `vessel_history`, `vessel_verbose` and `weather` data into a form useful for modeling\n",
    "- Transform the columns in new ones we can use for modeling\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferry_trips = vessel_history.select(\n",
    "    pl.col(\"Vessel\", \"Departing\", \"Arriving\"),\n",
    "    (pl.col(\"ActualDepart\") - pl.col(\"ScheduledDepart\"))\n",
    "    .dt.total_seconds()\n",
    "    .alias(\"Delay\"),\n",
    "    pl.col(\"Date\"),\n",
    "    pl.col(\"Date\").dt.year().alias(\"Year\"),\n",
    "    pl.col(\"Date\").dt.month().alias(\"Month\"),\n",
    "    pl.col(\"Date\").dt.weekday().alias(\"Weekday\"),\n",
    "    pl.col(\"Date\").dt.hour().alias(\"Hour\"),\n",
    ")\n",
    "\n",
    "ferry_trips.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the `Delay` data shows that there's significant skew and even some negative delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferry_trips.plot.hist(\"Delay\", bin_range=(-1800, 7200), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For the purposes of making it easier to model we'll assume delays can only be non-negative and log them in order to get a nicer distribution for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferry_trips = ferry_trips.select(\n",
    "    pl.exclude(\"Delay\"),\n",
    "    pl.col(\"Delay\")\n",
    "    .map_elements(lambda x: max(x, 1), return_dtype=pl.Float64)\n",
    "    .log()\n",
    "    .alias(\"LogDelay\"),\n",
    ")\n",
    "\n",
    "ferry_trips.plot.hist(\"LogDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll want to join the ferry data describing the vessels the trips were taken in. First we're selecting a subset of the columns and extracting the year from the `YearBuilt` and `YearRebuilt` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferry_info = vessel_verbose.select(\n",
    "    pl.col(\"VesselName\").str.to_lowercase(),\n",
    "    pl.col(\"ClassName\"),\n",
    "    pl.col(\n",
    "        \"SpeedInKnots\",\n",
    "        \"EngineCount\",\n",
    "        \"Horsepower\",\n",
    "        \"MaxPassengerCount\",\n",
    "        \"PassengerOnly\",\n",
    "        \"FastFerry\",\n",
    "        \"PropulsionInfo\",\n",
    "    ),\n",
    "    pl.col(\"YearBuilt\", \"YearRebuilt\").dt.year(),\n",
    ")\n",
    "\n",
    "ferry_trips = ferry_trips.join(\n",
    "    ferry_info, left_on=\"Vessel\", right_on=\"VesselName\", how=\"left\", coalesce=True\n",
    ")\n",
    "\n",
    "ferry_trips.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather data has a granularity of one hour, so in order to join this with the `ferry_trips` data we're going to round the timestamp associated with the trip to the nearest hour. We're going to join in the weather data twice for both the departing terminal and arriving terminal. Finally, there a number of columns associated with the weather data that are not needed and will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "\n",
    "ferry_trips = (\n",
    "    ferry_trips.with_columns(pl.col(\"Date\").dt.round(\"1h\").alias(\"time\"))\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"departing_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Departing\", \"time\"],\n",
    "        right_on=[\"departing_terminal_name\", \"departing_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"arriving_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Arriving\", \"time\"],\n",
    "        right_on=[\"arriving_terminal_name\", \"arriving_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .select(\n",
    "        ~cs.ends_with(\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"generationtime_ms\",\n",
    "            \"utc_offset_seconds\",\n",
    "            \"timezone\",\n",
    "            \"timezone_abbreviation\",\n",
    "            \"elevation\",\n",
    "            \"hourly_units\",\n",
    "        ),\n",
    "    )\n",
    "    .select(pl.exclude(\"time\"))\n",
    ")\n",
    "\n",
    "ferry_trips.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Preprocessing and Modeling\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "Define a `scikit-learn` pipeline that\n",
    "\n",
    "- Transform the data for the model to ingest\n",
    "- Trains a random forest model to predict the logged departure delay\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we separate the columns in numeric features and categorical features. Our random forest model requires the categorical features be one-hot encoded while our numeric features can be left as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_features = [\n",
    "    \"Month\",\n",
    "    \"Weekday\",\n",
    "    \"Hour\",\n",
    "    \"SpeedInKnots\",\n",
    "    \"EngineCount\",\n",
    "    \"Horsepower\",\n",
    "    \"MaxPassengerCount\",\n",
    "    # \"PassengerOnly\",\n",
    "    # \"FastFerry\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRebuilt\",\n",
    "    \"departing_temperature_2m\",\n",
    "    # \"departing_precipitation\",\n",
    "    \"departing_cloud_cover\",\n",
    "    \"departing_wind_speed_10m\",\n",
    "    \"departing_wind_direction_10m\",\n",
    "    \"departing_wind_gusts_10m\",\n",
    "    \"arriving_temperature_2m\",\n",
    "    # \"arriving_precipitation\",\n",
    "    \"arriving_cloud_cover\",\n",
    "    \"arriving_wind_speed_10m\",\n",
    "    \"arriving_wind_direction_10m\",\n",
    "    \"arriving_wind_gusts_10m\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Departing\",\n",
    "    \"Arriving\",\n",
    "    \"ClassName\",\n",
    "    \"PropulsionInfo\",\n",
    "    \"departing_weather_code\",\n",
    "    \"arriving_weather_code\",\n",
    "]\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(verbose=2, random_state=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our preprocessor and random forest model are joined together into a single pipeline. This makes using the model easier as we won't have to feed in pre-processed data - the pipeline will take of that step for us during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([(\"preprocess\", preprocessor), (\"random-forest\", rf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're filtering the data, keeping only the data from the past year, and then splitting into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ferry_trips_filtered = ferry_trips.drop_nulls().filter(\n",
    "    pl.col(\"Date\").dt.date() >= (datetime.date.today() - datetime.timedelta(weeks=53))\n",
    ")\n",
    "\n",
    "X = ferry_trips_filtered.drop(\"Vessel\", \"Date\", \"Year\", \"LogDelay\")\n",
    "\n",
    "# TODO: review issues with prototype data\n",
    "X = X.drop(\n",
    "    \"PassengerOnly\",\n",
    "    \"FastFerry\",\n",
    "    \"arriving_precipitation\",\n",
    "    \"departing_precipitation\",\n",
    ")\n",
    "y = ferry_trips_filtered[\"LogDelay\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "print(f\"Nrows training data: {X_train.shape[0]}\")\n",
    "print(f\"Nrows testing data:  {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the test later for our model card (to be discussed) the test data will be saved to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.with_columns(y_test).write_database(\n",
    "    table_name=f\"{username}_test_data\",\n",
    "    connection=db_uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model and compute the r-squared using the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train.to_pandas(), y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Deploying\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Deploy the model using `vetiver` and `pins` onto Posit Connect\n",
    "- Deploy an API around the model onto Posit\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vetiver import VetiverModel\n",
    "\n",
    "v = VetiverModel(\n",
    "    model, model_name=f\"{username}/ferry_delay\", prototype_data=X.to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pins\n",
    "import vetiver\n",
    "\n",
    "model_board = pins.board_connect(\n",
    "    server_url=connect_url, api_key=connect_api_key, allow_pickle_read=True\n",
    ")\n",
    "vetiver.vetiver_pin_write(model_board, model=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rsconnect.api import RSConnectServer\n",
    "\n",
    "connect_server = RSConnectServer(url=connect_url, api_key=connect_api_key)\n",
    "vetiver.deploy_rsconnect(\n",
    "    connect_server=connect_server,\n",
    "    board=model_board,\n",
    "    pin_name=f\"{username}/ferry_delay\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Model Card\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Use a model card to describe various metrics for how the model performs\n",
    "- Deploy the card to Connect\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetiver.templates.model_card()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
