{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: today\n",
    "execute:\n",
    "    enabled: true\n",
    "format:\n",
    "    email:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this exercise we will perform feature engineering on our training set then train a model, deploy it and write a [model card](https://doi.org/10.1145/3287560.3287596) to provide reporting on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Setup\n",
    "\n",
    "These are just some preliminary steps for getting started with this section.\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Load any environment variables located in `.env`\n",
    "- Get the username for content deployed to Posit Connect\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if Path(\".env\").exists():\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from posit.connect import Client\n",
    "\n",
    "with Client() as client:\n",
    "    username = client.me.username\n",
    "\n",
    "print(f\"Connect username is: '{username}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Reading the data\n",
    "\n",
    "First we will read in the data prepared in [02-data-exploration-and-validation](../02-data-exploration-and-validation/notebook.ipynb) and take a quick look at it prepare for modeling.\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Read in and glimpse the vessel history data\n",
    "- Read in and glimpse the vessel verbose data\n",
    "- Read in and glimpse the weather data\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "db_uri = os.environ[\"DATABASE_URI_PYTHON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "vessel_history = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_history_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_history.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_verbose_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_verbose.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_weather_clean;\", uri=db_uri, engine=\"adbc\"\n",
    ")\n",
    "\n",
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Feature Engineering\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Join the `vessel_history`, `vessel_verbose` and `weather` data into a form useful for modeling\n",
    "- Transform the columns in new ones we can use for modeling\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute a departure delay based on the difference between the actual and scheduled departure times and derive new features representing the day of the week and hour of departure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = vessel_history.with_columns(\n",
    "    (pl.col(\"ActualDepart\") - pl.col(\"ScheduledDepart\"))\n",
    "    .dt.total_seconds()\n",
    "    .alias(\"Delay\"),\n",
    "    pl.col(\"Date\").dt.weekday().alias(\"Weekday\"),\n",
    "    pl.col(\"Date\").dt.hour().alias(\"Hour\"),\n",
    ").drop(\"EstArrival\")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at `Delay` shows it is very skewed with a lot of values near zero and some even less than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined.plot.hist(\"Delay\", bin_range=(-1800, 7200), bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not interested in negative values here, we'll clip the values below zero. We can then log them, turning into it a nicer, more \"normal\" distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.select(\n",
    "    pl.exclude(\"Delay\"),\n",
    "    pl.col(\"Delay\")\n",
    "    .map_elements(lambda x: max(x, 1), return_dtype=pl.Float64)\n",
    "    .log()\n",
    "    .alias(\"LogDelay\"),\n",
    ")\n",
    "\n",
    "trips_combined.plot.hist(\"LogDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the data describing the individual vessels by only keeping the columns we're interested in and extracting the year from `YearBuilt` and `YearRebuilt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_info = vessel_verbose.select(\n",
    "    pl.col(\"VesselName\"),\n",
    "    pl.col(\"ClassName\"),\n",
    "    # we can also select multiple columns in one `pl.col(...)`\n",
    "    pl.col(\n",
    "        \"SpeedInKnots\",\n",
    "        \"EngineCount\",\n",
    "        \"Horsepower\",\n",
    "        \"MaxPassengerCount\",\n",
    "        \"PassengerOnly\",\n",
    "        \"FastFerry\",\n",
    "        \"PropulsionInfo\",\n",
    "    ),\n",
    "    pl.col(\"YearBuilt\", \"YearRebuilt\").dt.year(),\n",
    ")\n",
    "\n",
    "vessel_info.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trips and vessel data are joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.join(\n",
    "    vessel_info, left_on=\"Vessel\", right_on=\"VesselName\", how=\"left\", coalesce=True\n",
    ")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining in the weather data is a little harder since the weather data has a temporal resolution of one-hour whereas the trips can leave at any arbitrary time, making a naive join on timestamps not work. Here, we round the the departure time of the trips to the nearest hour and then join on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.select(\n",
    "    pl.col(\n",
    "        \"time\",\n",
    "        \"weather_code\",\n",
    "        \"temperature_2m\",\n",
    "        \"precipitation\",\n",
    "        \"cloud_cover\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"wind_gusts_10m\",\n",
    "        \"terminal_name\",\n",
    "    )\n",
    ")\n",
    "\n",
    "trips_combined = (\n",
    "    trips_combined.with_columns(pl.col(\"Date\").dt.round(\"1h\").alias(\"time\"))\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"departing_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Departing\", \"time\"],\n",
    "        right_on=[\"departing_terminal_name\", \"departing_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .join(\n",
    "        weather.rename(lambda col_name: f\"arriving_{col_name}\"),\n",
    "        how=\"left\",\n",
    "        left_on=[\"Arriving\", \"time\"],\n",
    "        right_on=[\"arriving_terminal_name\", \"arriving_time\"],\n",
    "        coalesce=True,\n",
    "    )\n",
    "    .select(pl.exclude(\"time\"))\n",
    ")\n",
    "\n",
    "trips_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how many `null`s there are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the drop the `null`s except for `YearRebuilt` since we expect to have weather data available. We can expect `YearRebuilt` in the case of vessels that have not yet been rebuilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "\n",
    "trips_combined = trips_combined.drop_nulls(subset=cs.exclude(\"YearRebuilt\"))\n",
    "\n",
    "trips_combined.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we enumerate all our numerical and categorical features. This will be needed later in setting up preprocessing for our model. For the categorical features we take a count of how often each category occurs to make sure our model isn't dependant on categories that are uncommon in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"SpeedInKnots\",\n",
    "    \"EngineCount\",\n",
    "    \"Horsepower\",\n",
    "    \"MaxPassengerCount\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRebuilt\",\n",
    "    \"departing_temperature_2m\",\n",
    "    \"departing_cloud_cover\",\n",
    "    \"departing_wind_speed_10m\",\n",
    "    \"departing_wind_direction_10m\",\n",
    "    \"departing_wind_gusts_10m\",\n",
    "    \"arriving_temperature_2m\",\n",
    "    \"arriving_cloud_cover\",\n",
    "    \"arriving_wind_speed_10m\",\n",
    "    \"arriving_wind_direction_10m\",\n",
    "    \"arriving_wind_gusts_10m\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Vessel\",\n",
    "    \"Weekday\",\n",
    "    \"Hour\",\n",
    "    \"Departing\",\n",
    "    \"Arriving\",\n",
    "    \"ClassName\",\n",
    "    \"PropulsionInfo\",\n",
    "    \"departing_weather_code\",\n",
    "    \"arriving_weather_code\",\n",
    "]\n",
    "\n",
    "for cf in categorical_features:\n",
    "    print(\n",
    "        f\"feature: '{cf}', count:\",\n",
    "        trips_combined.group_by(cf).agg(pl.len()).sort(\"len\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the weather codes don't occur as frequently as would be ideal and decide to keep note of which codes these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_count_weather_codes = set(\n",
    "    [\n",
    "        *trips_combined.group_by(\"departing_weather_code\")\n",
    "        .agg(pl.len())\n",
    "        .sort(\"len\")\n",
    "        .filter(pl.col(\"len\") < 300)[\"departing_weather_code\"]\n",
    "        .to_list(),\n",
    "        *trips_combined.group_by(\"arriving_weather_code\")\n",
    "        .agg(pl.len())\n",
    "        .sort(\"len\")\n",
    "        .filter(pl.col(\"len\") < 300)[\"arriving_weather_code\"]\n",
    "        .to_list(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "low_count_weather_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These less common weather codes are binned into their own category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_weather_codes(code):\n",
    "    return \"other\" if code in low_count_weather_codes else str(code)\n",
    "\n",
    "\n",
    "trips_combined = trips_combined.with_columns(\n",
    "    pl.col(\"departing_weather_code\").map_elements(\n",
    "        recode_weather_codes, return_dtype=pl.String\n",
    "    ),\n",
    "    pl.col(\"arriving_weather_code\").map_elements(\n",
    "        recode_weather_codes, return_dtype=pl.String\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some unnecessary columns are dropped and the `Date` column is turned into a proper date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_combined = trips_combined.select(\n",
    "    cs.exclude(\"ScheduledDepart\", \"ActualDepart\")\n",
    ").with_columns(pl.col(\"Date\").dt.date())\n",
    "\n",
    "trips_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Model Training\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "Define a `scikit-learn` pipeline that\n",
    "\n",
    "- Transform the data for the model to ingest\n",
    "- Trains a gradient boosted machine model to predict the logged departure delay\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a column transformer is defined to preprocess our data. Numerical and categorical columns are treated differently with the former being passed through as-is and the latter being one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        # this just passes the variables through as-is\n",
    "        (\"numeric_features\", \"passthrough\", numeric_features),\n",
    "        # this one-hot encodes the variables\n",
    "        (\"categorical_features\", OneHotEncoder(), categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define our actual model, a [gradient boosted machine](https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting) akin to [LightGBM](https://github.com/Microsoft/LightGBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "regressor = HistGradientBoostingRegressor(verbose=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model requires non-sparse data, so a custom transformer is defined to take a sparse dataset and turn it into a dense one. See [toarray](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class DenseTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None, **params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transformers and model are combined into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline(\n",
    "    [\n",
    "        (\"column-transformer\", column_transformer),\n",
    "        (\"densify\", DenseTransformer()),\n",
    "        (\"regressor\", regressor),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set aside more recent for [model monitoring](../04-model-monitoring/monitoring_dashboard.qmd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train_test_data = trips_combined.filter(\n",
    "    pl.col(\"Date\") < (datetime.date.today() - datetime.timedelta(weeks=4))\n",
    ")\n",
    "\n",
    "monitoring_data = trips_combined.filter(\n",
    "    pl.col(\"Date\") >= (datetime.date.today() - datetime.timedelta(weeks=4))\n",
    ")\n",
    "\n",
    "monitoring_data.write_database(\n",
    "    table_name=f\"{username}_monitoring_data\",\n",
    "    connection=db_uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now the remaining data is split into a training set and testing set for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_test_data.drop(\"LogDelay\", \"Date\")\n",
    "y = train_test_data[\"LogDelay\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "print(f\"Number of rows of training data: {X_train.shape[0]}\")\n",
    "print(f\"Number of rows testing data:  {X_test.shape[0]}\")\n",
    "\n",
    "X_test.with_columns(y_test).write_database(\n",
    "    table_name=f\"{username}_test_data\",\n",
    "    connection=db_uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, our model is trained. This also trains the encodings for our one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train.to_pandas(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Model Deployment\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Deploy the model using `vetiver` and `pins` onto Posit Connect\n",
    "- Deploy an API around the model onto Posit\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is wrapped up in a [`VetiverModel` object for serving it](https://rstudio.github.io/vetiver-python/stable/reference/VetiverModel.html#vetiver.VetiverModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vetiver import VetiverModel\n",
    "\n",
    "v = VetiverModel(\n",
    "    model, model_name=f\"{username}/ferry_delay\", prototype_data=X.to_pandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now deployed to Connect, making it available to use elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pins\n",
    "import vetiver\n",
    "\n",
    "model_board = pins.board_connect(allow_pickle_read=True)\n",
    "vetiver.vetiver_pin_write(model_board, model=v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to more easily use our model, we need to define an API that loads and serves it. This API will also be deployed to Connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "Path(\"api\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# vetiver.write_app(model_board, f\"{username}/ferry_delay\", file=\"api/app.py\")\n",
    "shutil.copy2(\"requirements.txt\", \"api/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "api_guid = os.getenv(\"API_GUID\")\n",
    "if api_guid is None:\n",
    "    !rsconnect deploy fastapi --entrypoint app:api --title \"{username}/ferry_delay_vetiver\" api/\n",
    "else:\n",
    "    !rsconnect deploy fastapi --app-id \"{api_guid}\" --entrypoint app:api --title \"{username}/ferry_delay_vetiver\" api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Model Card\n",
    "\n",
    "### üîÑ Task\n",
    "\n",
    "- Use a model card to describe various metrics for how the model performs\n",
    "- Deploy the card to Connect\n",
    "\n",
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetiver.templates.model_card()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
